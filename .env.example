# vLLM Configuration
# For GPUs with 16GB VRAM, use 7B models
# For GPUs with 24GB VRAM, you can try 13B models
MODEL_NAME=Qwen/Qwen2.5-7B-Instruct
MAX_MODEL_LEN=8192
GPU_MEMORY_UTILIZATION=0.95
DTYPE=auto

# API Keys (change these to secure values!)
VLLM_API_KEY=your-vllm-api-key-here
WEBUI_SECRET_KEY=your-webui-secret-key-here

# Open WebUI Settings
ENABLE_SIGNUP=false
DEFAULT_USER_ROLE=pending
ENABLE_COMMUNITY_SHARING=false

# Optional: Hugging Face token for gated models
HUGGING_FACE_TOKEN=

# PyTorch memory optimization
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True