# vLLM Configuration
# For GPUs with 16GB VRAM, use 7B models
# For GPUs with 24GB VRAM, you can try 13B models

# Model Options (use select_model.py for easy switching):
# - Qwen/Qwen2.5-7B-Instruct (default, excellent general purpose)
# - mistralai/Mistral-7B-Instruct-v0.3 (strong reasoning)
# - mistralai/Mistral-7B-Instruct-v0.2 (previous version)
# - meta-llama/Meta-Llama-3-8B-Instruct (requires HF token)
# - microsoft/Phi-3-mini-4k-instruct (lower VRAM, ~8GB)

MODEL_NAME=Qwen/Qwen2.5-7B-Instruct
MAX_MODEL_LEN=8192
GPU_MEMORY_UTILIZATION=0.95
DTYPE=auto

# API Keys (change these to secure values!)
VLLM_API_KEY=your-vllm-api-key-here
WEBUI_SECRET_KEY=your-webui-secret-key-here

# Open WebUI Settings
ENABLE_SIGNUP=false
DEFAULT_USER_ROLE=pending
ENABLE_COMMUNITY_SHARING=false

# Optional: Hugging Face token for gated models
HUGGING_FACE_TOKEN=

# PyTorch memory optimization
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True